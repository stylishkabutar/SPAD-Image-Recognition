{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":97555,"databundleVersionId":11670858,"sourceType":"competition"},{"sourceId":11328026,"sourceType":"datasetVersion","datasetId":7086031}],"dockerImageVersionId":31013,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"name":"notebook7a00bcef80","provenance":[],"gpuType":"V28"},"accelerator":"TPU","widgets":{"application/vnd.jupyter.widget-state+json":{"cad36ef3ab4d43fab1ff6913fff0b460":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_cf9feb6651d442218d5cce5b119de6e2"],"layout":"IPY_MODEL_3d981e9fb0bc4d509a122fb84aeac660"}},"f25a8cda9b69411fb316e65be059759d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ca4d20d56cf4eaba048e197372e6a2e","placeholder":"​","style":"IPY_MODEL_87c534b36e25483290d25838c1fdb7ff","value":"<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"}},"be446e54fc5d4d96a38ec588f9122070":{"model_module":"@jupyter-widgets/controls","model_name":"TextModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"TextModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"TextView","continuous_update":true,"description":"Username:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_8863b3f86c534b858be2c1e601aa464a","placeholder":"​","style":"IPY_MODEL_f9f0f75dcedd40279f8fcac49ef28982","value":"stylishkabutar"}},"b5ab06d1b48049e79c9b52990605fe15":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_b30c529c411941bb8f78e9a41f361544","placeholder":"​","style":"IPY_MODEL_cf0e306c22024c2ca870346ba305f50a","value":""}},"57f12bac3beb43ca92d684ff43eef84d":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_55d19726d02c412ab38c1cf2f7090b16","style":"IPY_MODEL_273a9915d1ff48e8af60a9bc2377cfe2","tooltip":""}},"b58a95cecd744f71a29e6039ab0a2db6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a5479491c614ac2928424b270779057","placeholder":"​","style":"IPY_MODEL_eb6b5463258445ce9ab82a51f53bf031","value":"\n<b>Thank You</b></center>"}},"3d981e9fb0bc4d509a122fb84aeac660":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"5ca4d20d56cf4eaba048e197372e6a2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87c534b36e25483290d25838c1fdb7ff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8863b3f86c534b858be2c1e601aa464a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9f0f75dcedd40279f8fcac49ef28982":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b30c529c411941bb8f78e9a41f361544":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf0e306c22024c2ca870346ba305f50a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"55d19726d02c412ab38c1cf2f7090b16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"273a9915d1ff48e8af60a9bc2377cfe2":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"8a5479491c614ac2928424b270779057":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb6b5463258445ce9ab82a51f53bf031":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"70c4c4d1a3f74a23a62469d8bf033c97":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd1667463bdb4bc395b129e7f69c91e7","placeholder":"​","style":"IPY_MODEL_618f0e1fd7df437daa384c14d6104048","value":"Connecting..."}},"bd1667463bdb4bc395b129e7f69c91e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"618f0e1fd7df437daa384c14d6104048":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cf9feb6651d442218d5cce5b119de6e2":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e97c1bd44314ef8a029d413459166f1","placeholder":"​","style":"IPY_MODEL_1a22ce07ce27482bac0afd3d57b4352f","value":"Kaggle credentials successfully validated."}},"0e97c1bd44314ef8a029d413459166f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a22ce07ce27482bac0afd3d57b4352f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n# # RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n# import kagglehub\n\n\n\n\n# kagglehub.login()\n","metadata":{"id":"pk1U3j2CFCTb","outputId":"b91d0cf8-3e04-41fa-ba2d-0701ffe6ad1f","trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:03:47.672333Z","iopub.status.idle":"2025-04-29T09:03:47.673102Z","shell.execute_reply.started":"2025-04-29T09:03:47.672527Z","shell.execute_reply":"2025-04-29T09:03:47.672544Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n# # THEN FEEL FREE TO DELETE THIS CELL.\n# # NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n# # ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n# # NOTEBOOK.\n\n# ee_5179_modern_computer_vision_course_competition_path = kagglehub.competition_download('ee-5179-modern-computer-vision-course-competition')\n# stylishkabutar_bsds500_path = kagglehub.dataset_download('stylishkabutar/bsds500')\n\n# print('Data source import complete.')\n","metadata":{"id":"f3Wo0EvEFCTg","outputId":"64888d93-7b88-4c29-985a-f0229be03713","trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:03:47.674129Z","iopub.status.idle":"2025-04-29T09:03:47.674646Z","shell.execute_reply.started":"2025-04-29T09:03:47.674277Z","shell.execute_reply":"2025-04-29T09:03:47.674291Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ee_5179_modern_computer_vision_course_competition_path","metadata":{"id":"RO6vxslAhRlO","outputId":"943bf8fb-3adb-46c3-d187-c6da15773a63","trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:03:47.675217Z","iopub.status.idle":"2025-04-29T09:03:47.675973Z","shell.execute_reply.started":"2025-04-29T09:03:47.675355Z","shell.execute_reply":"2025-04-29T09:03:47.675368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# file=('/kaggle/input')\n# print(os.listdir(file))","metadata":{"id":"a_nR76sFHEd3","outputId":"fc2736a3-1310-4865-e8f0-33eeae5a3ce1","trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:03:47.677581Z","iopub.status.idle":"2025-04-29T09:03:47.678439Z","shell.execute_reply.started":"2025-04-29T09:03:47.678127Z","shell.execute_reply":"2025-04-29T09:03:47.678143Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms,models\nfrom PIL import Image\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n# from scipy.io import loadmat\n# from skimage import io\n# import cv2","metadata":{"trusted":true,"id":"dubr4N1IFCTh","outputId":"e311fc75-43ad-47c2-c431-b8595276c794","execution":{"iopub.status.busy":"2025-04-29T09:03:47.680001Z","iopub.status.idle":"2025-04-29T09:03:47.680752Z","shell.execute_reply.started":"2025-04-29T09:03:47.680159Z","shell.execute_reply":"2025-04-29T09:03:47.680174Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from datasets import load_dataset\n\n# dataset = load_dataset(\"0jl/NYUv2\", trust_remote_code=True, split=\"train\")\n","metadata":{"trusted":true,"id":"i55KMtQqFCTi","execution":{"iopub.status.busy":"2025-04-29T09:03:47.681343Z","iopub.status.idle":"2025-04-29T09:03:47.682045Z","shell.execute_reply.started":"2025-04-29T09:03:47.681472Z","shell.execute_reply":"2025-04-29T09:03:47.681485Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# from PIL import Image\n# from torchvision import datasets\n\n# # Assuming 'dataset' is your PyTorch dataset\n#   # Replace with your actual dataset\n\n# # Define the target directory\n# target_directory = 'nyu2'\n# gt_directory='nyu2l'\n# os.makedirs(target_directory, exist_ok=True)\n\n# # Loop through the dataset and save images\n# for idx, outputs in enumerate(dataset):\n#     image_path = os.path.join(target_directory, f'image_{idx}.png')\n#     outputs['image'].save(image_path)\n#     # image_path = os.path.join(gt_directory, f'image_{idx}.png')\n#     # outputs['depth'].save(image_path)# Save the image using PIL\n\n# print(\"Images have been successfully saved to the target folder!\")\n","metadata":{"trusted":true,"id":"zrs2SV75FCTj","execution":{"iopub.status.busy":"2025-04-29T09:03:47.683038Z","iopub.status.idle":"2025-04-29T09:03:47.683489Z","shell.execute_reply.started":"2025-04-29T09:03:47.683181Z","shell.execute_reply":"2025-04-29T09:03:47.683195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# max((dataset[0]['depth']))\n","metadata":{"trusted":true,"id":"nLazkHqHFCTj","execution":{"iopub.status.busy":"2025-04-29T09:03:47.684137Z","iopub.status.idle":"2025-04-29T09:03:47.684777Z","shell.execute_reply.started":"2025-04-29T09:03:47.684276Z","shell.execute_reply":"2025-04-29T09:03:47.684289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"id":"uY0WYaPCFCTk"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ### Code to create synthetic Binary Images from RGB Images ####\n# ### Just input the folder in BASE and it will output the binary images in BASE__1 ###\n# import numpy as np\n# import os, sys\n# import cv2\n# # Original Clean Images\n# BASE = 'nyu2'\n# frames = [1]\n\n# TOTAL_FRAMES=frames[-1]\n# imgs = sorted(os.listdir(BASE))\n# for idx in range(len(imgs)):\n#     im = cv2.imread(os.path.join(BASE, imgs[idx]))\n#     im = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n#     count_shape = im.shape + (-1,)\n\n#     for i in range(1):\n#         # Simulate low light conditions by scaling pixel values by 1/1000.\n#         # Poisson process to simulate photon arrival\n#         photon_counts = np.random.poisson(im.flatten()/255., size=(TOTAL_FRAMES, im.size)).T\n#         photon_counts = photon_counts.reshape(count_shape)\n#         # Photon counts is converted to binary frames\n\n#         b_counts=np.where(photon_counts>=1, 1, 0)\n#         for fil in frames:\n#             recon_image = np.mean(b_counts[:,:,0:fil], axis=2)\n#             #outfile = BASENEW.replace('60', str(fil)) + str(i) + im_name.replace('JPEG', 'png')\n#             outfile = os.path.join(BASE+\"__\"+str(fil), imgs[idx])\n#             directory = os.path.dirname(outfile)\n#             if not os.path.exists(directory):\n#                 os.makedirs(directory)\n\n#             # recon_image = np.repeat(np.expand_dims(recon_image, axis = -1), 3, axis = -1)\n#             recon_image = np.expand_dims(recon_image, axis = -1)\n#             cv2.imwrite(outfile, recon_image*255.)","metadata":{"trusted":true,"id":"D2oPVEE_FCTk","execution":{"iopub.status.busy":"2025-04-29T09:03:47.685403Z","iopub.status.idle":"2025-04-29T09:03:47.686255Z","shell.execute_reply.started":"2025-04-29T09:03:47.685533Z","shell.execute_reply":"2025-04-29T09:03:47.685546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plt.imshow(plt.imread('/kaggle/working/nyu2__1/image_0.png'))","metadata":{"trusted":true,"id":"Cl8E5nmlFCTl","execution":{"iopub.status.busy":"2025-04-29T09:03:47.854666Z","iopub.execute_input":"2025-04-29T09:03:47.855025Z","iopub.status.idle":"2025-04-29T09:03:47.867640Z","shell.execute_reply.started":"2025-04-29T09:03:47.854998Z","shell.execute_reply":"2025-04-29T09:03:47.863421Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# from IPython.display import FileLink\n# FileLink(\"predictions_hed_resss_cub7.csv\")","metadata":{"trusted":true,"id":"k_LVR0xIFCTm","outputId":"fb340259-7677-40e2-b0fe-5d15c9b59456","execution":{"iopub.status.busy":"2025-04-29T09:03:47.870007Z","iopub.execute_input":"2025-04-29T09:03:47.870258Z","iopub.status.idle":"2025-04-29T09:03:47.888299Z","shell.execute_reply.started":"2025-04-29T09:03:47.870233Z","shell.execute_reply":"2025-04-29T09:03:47.882745Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# import torch\n# from torchvision import transforms\n# from torchvision.datasets import ImageFolder\n# from torch.utils.data import DataLoader\n# from PIL import Image\n# import numpy as np\n\n# # Define multiscale Gaussian filters\n# scales = [1, 2, 3, 5]  # Different sigma values\n# filters = [transforms.GaussianBlur(kernel_size=(2*s+1, 2*s+1), sigma=s) for s in scales]\n\n# # Custom transform to apply Gaussian filters and stack as channels\n# class MultiscaleGaussianTransform:\n#     def __init__(self, filters):\n#         self.filters = filters\n\n#     def __call__(self, img):\n#         # Apply each filter and stack results as channels\n#         filtered_images = [f(img) for f in self.filters]\n#         filtered_images.append(img)\n#         stacked_image = torch.stack([transforms.ToTensor()(fi).squeeze(0) for fi in filtered_images], dim=0)\n#         return stacked_image\n\n# # Dataset and DataLoader\n# dataset_path = \"path/to/your/dataset\"  # Replace with your dataset path\n\n# transform = MultiscaleGaussianTransform(filters)  # Apply your custom multiscale transform\n\n# # dataset = ImageFolder(root=dataset_path, transform=transform)\n# # dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n\n# # # Process the dataset\n# # for batch_idx, (images, labels) in enumerate(dataloader):\n# #     print(f\"Batch {batch_idx}: {images.shape}\")  # Shape: [batch_size, num_filters, C, H, W]\n","metadata":{"trusted":true,"id":"Jeq0QfQKFCTn","execution":{"iopub.status.busy":"2025-04-29T09:03:47.889927Z","iopub.execute_input":"2025-04-29T09:03:47.890335Z","iopub.status.idle":"2025-04-29T09:03:47.903366Z","shell.execute_reply.started":"2025-04-29T09:03:47.890311Z","shell.execute_reply":"2025-04-29T09:03:47.899029Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# import os\n\n# # Create the directory\n# os.makedirs('/kaggle/working/test_fold', exist_ok=True)\n","metadata":{"trusted":true,"id":"_rUnxJOeFCTo","execution":{"iopub.status.busy":"2025-04-29T09:03:47.905334Z","iopub.execute_input":"2025-04-29T09:03:47.905541Z","iopub.status.idle":"2025-04-29T09:03:47.919493Z","shell.execute_reply.started":"2025-04-29T09:03:47.905522Z","shell.execute_reply":"2025-04-29T09:03:47.915473Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"Here we see that the canny detector does extract very sharp edges, but it either msises edges, or extracts too many, as it extracts edges at a particular scale","metadata":{"id":"VxBLJsdAFCTo"}},{"cell_type":"code","source":"class BSDS500Dataset(Dataset):\n    def __init__(self, image_dir, gt_dir, train_transform=None,test_transform=None):\n        self.image_paths =os.listdir(image_dir)\n\n\n        self.image_dir=image_dir\n        self.gt_dir=gt_dir\n        self.test_transform = test_transform\n        self.train_transform=train_transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        # Load image\n        image = Image.open(self.image_dir+'/'+self.image_paths[idx])\n        gt = Image.open(self.gt_dir+'/'+self.image_paths[idx])\n        # Load ground truth\n\n        if image.size[0]!=gt.size[0]:\n            gt = gt.transpose(Image.ROTATE_90)\n        if image.size[0]>image.size[1]:\n            image = image.transpose(Image.ROTATE_90)\n            gt = gt.transpose(Image.ROTATE_90)\n\n        # Apply transformations\n        if self.train_transform:\n            image = self.train_transform(image)\n        if self.test_transform:\n            gt = self.test_transform(gt)\n\n        return image, gt","metadata":{"trusted":true,"id":"_BgP0VOUFCTq","execution":{"iopub.status.busy":"2025-04-29T09:03:47.921630Z","iopub.execute_input":"2025-04-29T09:03:47.921836Z","iopub.status.idle":"2025-04-29T09:03:47.961687Z","shell.execute_reply.started":"2025-04-29T09:03:47.921816Z","shell.execute_reply":"2025-04-29T09:03:47.958096Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mBSDS500Dataset\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_dir, gt_dir, train_transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,test_transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths \u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mlistdir(image_dir)\n","\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"],"ename":"NameError","evalue":"name 'Dataset' is not defined","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n","metadata":{"id":"Nzp05K3JiVNq","outputId":"3c9be3f5-6753-4af0-be91-e1680d665ca5","trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:03:47.964829Z","iopub.status.idle":"2025-04-29T09:03:47.966644Z","shell.execute_reply.started":"2025-04-29T09:03:47.966126Z","shell.execute_reply":"2025-04-29T09:03:47.966145Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch-xla\n!pip install cloud-tpu-client\n","metadata":{"id":"0Y2pdI2lifwi","outputId":"f957c1c2-3323-48e9-a104-93671192a1da","trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:03:47.968887Z","iopub.status.idle":"2025-04-29T09:03:47.970970Z","shell.execute_reply.started":"2025-04-29T09:03:47.969327Z","shell.execute_reply":"2025-04-29T09:03:47.969344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch_xla\nimport torch_xla.core.xla_model as xm\n\n# Send model to TPU device\ndevice = xm.xla_device()\n# model = model.to(device)\n","metadata":{"id":"sM_ZA5GQiRv3","trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:03:47.972624Z","iopub.status.idle":"2025-04-29T09:03:47.973071Z","shell.execute_reply.started":"2025-04-29T09:03:47.972789Z","shell.execute_reply":"2025-04-29T09:03:47.972805Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nbase_dir ='/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data'\ntrain_img_dir = os.path.join('/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/training-images')\ntrain_gt_dir = os.path.join('/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/training-depths')\nval_img_dir = os.path.join('/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/validation-images')\nval_gt_dir = os.path.join('/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/validation-depths')\ntest_img_dir = os.path.join('/kaggle/input/ee-5179-modern-computer-vision-course-competition/testing-images')\ntransform = transforms.ToTensor()\n\ntransform1 = transforms.ToTensor()\ntrain_dataset = BSDS500Dataset(train_img_dir, train_gt_dir, train_transform=transform,test_transform=transform1)\nval_dataset = BSDS500Dataset(val_img_dir, val_gt_dir, train_transform=transform,test_transform=transform1)\ntrain_sampler = torch.utils.data.distributed.DistributedSampler(\n    train_dataset, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal()\n)\nval_sampler = torch.utils.data.distributed.DistributedSampler(\n    val_dataset, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal()\n)\n# test_dataset = BSDS500Dataset(test_img_dir, test_gt_dir, transform=transform)\ntrain_loader = DataLoader(train_dataset, sampler=train_sampler)\nval_loader = DataLoader(val_dataset,sampler=val_sampler)\n# test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)","metadata":{"trusted":true,"id":"k0wAYSOfFCTq","outputId":"9d9df9bf-73e4-4212-ba75-67541e5a7b66","execution":{"iopub.status.busy":"2025-04-29T09:03:47.975552Z","iopub.status.idle":"2025-04-29T09:03:47.976572Z","shell.execute_reply.started":"2025-04-29T09:03:47.975999Z","shell.execute_reply":"2025-04-29T09:03:47.976017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class NYUDEPTHV2(Dataset):\n#     def __init__(self, image_dir, gt_dataset, train_transform=None,test_transform=None):\n#         self.image_paths =os.listdir(image_dir)\n\n\n#         self.image_dir=image_dir\n#         self.gt_dataset=gt_dataset\n#         self.test_transform = test_transform\n#         self.train_transform=train_transform\n\n#     def __len__(self):\n#         return len(self.image_paths)\n\n#     def __getitem__(self, idx):\n#         # Load image\n#         image = Image.open(self.image_dir+'/'+f'image_{idx}.png')\n\n#         # Load ground truth\n#         depth_array = np.array(self.gt_dataset[idx]['depth'])\n\n# # Normalize the depth values to fit within the range of 0-255\n#         depth_normalized = (depth_array - depth_array.min()) / (depth_array.max() - depth_array.min()) * 255\n#         depth_normalized = depth_normalized.astype(np.uint8)\n\n# # Convert the normalized depth array to a PIL image\n#         gt = Image.fromarray(depth_normalized)\n#         if image.size[0]!=gt.size[0]:\n#             gt = gt.transpose(Image.ROTATE_90)\n#         if image.size[0]>image.size[1]:\n#             image = image.transpose(Image.ROTATE_90)\n#             gt = gt.transpose(Image.ROTATE_90)\n#         image = image.resize((481, 321))\n#         gt = gt.resize((481, 321))\n#         # Apply transformations\n#         if self.train_transform:\n#             image = self.train_transform(image)\n#         if self.test_transform:\n#             gt = self.test_transform(gt)\n\n#         return image, gt","metadata":{"trusted":true,"id":"smfp7wwBFCTr","execution":{"iopub.status.busy":"2025-04-29T09:03:47.977301Z","iopub.status.idle":"2025-04-29T09:03:47.978018Z","shell.execute_reply.started":"2025-04-29T09:03:47.977693Z","shell.execute_reply":"2025-04-29T09:03:47.977709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_dataset_2=NYUDEPTHV2('/kaggle/working/nyu2__1',dataset, train_transform=transform,test_transform=transform1)","metadata":{"trusted":true,"id":"MiogwYNRFCTr","execution":{"iopub.status.busy":"2025-04-29T09:03:47.978947Z","iopub.status.idle":"2025-04-29T09:03:47.979721Z","shell.execute_reply.started":"2025-04-29T09:03:47.979137Z","shell.execute_reply":"2025-04-29T09:03:47.979153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_loader_2 = DataLoader(train_dataset_2, batch_size=4, shuffle=True)\n","metadata":{"trusted":true,"id":"3MpIBC-KFCTr","execution":{"iopub.status.busy":"2025-04-29T09:03:47.980373Z","iopub.status.idle":"2025-04-29T09:03:47.981303Z","shell.execute_reply.started":"2025-04-29T09:03:47.980518Z","shell.execute_reply":"2025-04-29T09:03:47.980533Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plt.imshow(train_dataset_2[0][1][0])","metadata":{"trusted":true,"id":"MgtGXhg4FCTr","execution":{"iopub.status.busy":"2025-04-29T09:03:47.983147Z","iopub.status.idle":"2025-04-29T09:03:47.984499Z","shell.execute_reply.started":"2025-04-29T09:03:47.983455Z","shell.execute_reply":"2025-04-29T09:03:47.983472Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# models.resnet34(pretrained=False)","metadata":{"trusted":true,"id":"-PW4ETitFCTr","execution":{"iopub.status.busy":"2025-04-29T09:03:47.986712Z","iopub.status.idle":"2025-04-29T09:03:47.987649Z","shell.execute_reply.started":"2025-04-29T09:03:47.986973Z","shell.execute_reply":"2025-04-29T09:03:47.986990Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nclass HED(nn.Module):\n    def __init__(self):\n        super(HED, self).__init__()\n\n        # Load pre-trained VGG16 features\n        features = models.resnet34(pretrained=False)\n\n        # Split VGG16 into blocks (until each pooling layer)\n        self.conv0 = nn.Conv2d(1,3, kernel_size=1)\n        self.conv1 = features.conv1\n        self.bn1 = features.bn1\n        self.rel = features.relu\n        self.mp = features.maxpool\n        self.conv11 = features.layer1  # until pool1\n        self.conv2 = features.layer2 # until pool2\n        self.conv3 = features.layer3 # until pool3\n        self.conv4 = features.layer4 # until pool4\n        self.conv5=features\n        # self.conv5 = features[23:30] # until pool5 (exclude pool5)\n        self.linear=nn.Linear(1000,1024)\n        # Side output layers\n        self.side1 = nn.Conv2d(64, 1, kernel_size=1)\n        self.side2 = nn.Conv2d(256, 1, kernel_size=1)\n        self.side3 = nn.Conv2d(256, 1, kernel_size=1)\n        self.side4 = nn.Conv2d(512, 1, kernel_size=1)\n\n        # self.side5 = nn.Conv2d(512, 1, kernel_size=1)\n\n        # Fusion layer\n        self.fusion = nn.Conv2d(5, 1, kernel_size=1)\n        self.decoder4= nn.Sequential(\n            nn.ConvTranspose2d(in_channels= 512,out_channels=256, kernel_size=4, stride=2, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(in_channels=256, out_channels=32, kernel_size=4, stride=2, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(in_channels=32, out_channels=8, kernel_size=4, stride=2, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(in_channels=8, out_channels=2, kernel_size=4, stride=2, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True),\n        nn.ConvTranspose2d(in_channels=2, out_channels=1, kernel_size=4, stride=2, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True))\n        self.decoder3= nn.Sequential(\n            nn.ConvTranspose2d(in_channels=256, out_channels=16, kernel_size=4, stride=2, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(in_channels=16, out_channels=4, kernel_size=4, stride=2, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(in_channels=4, out_channels=2, kernel_size=4, stride=2, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True),\n        nn.ConvTranspose2d(in_channels=2, out_channels=1, kernel_size=4, stride=2, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True))\n        self.decoder2=nn.Sequential(\n\n            nn.ConvTranspose2d(in_channels=256, out_channels=32, kernel_size=4, stride=2, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(in_channels=32, out_channels=8, kernel_size=4, stride=2, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True),\n        nn.ConvTranspose2d(in_channels=8, out_channels=1, kernel_size=4, stride=2, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True))\n        self.decoder1= nn.Sequential(\n\n            nn.ConvTranspose2d(in_channels=64, out_channels=8, kernel_size=4, stride=2, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True),\n        nn.ConvTranspose2d(in_channels=8, out_channels=1, kernel_size=4, stride=2, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True))\n        self.decoder5= nn.Sequential(\n\n            nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=4, stride=2, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True),\n        nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=4, stride=2, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True),\n        nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=4, stride=2, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True))\n        # Initialize weights for side outputs and fusion\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        h, w = x.size(2), x.size(3)\n\n        # VGG blocks forward pass\n        x=self.conv0(x)\n        side5=self.conv5(x)\n\n        x=self.conv1(x)\n        x=self.bn1(x)\n        x=self.rel(x)\n        x=self.mp(x)\n        conv1 = self.conv11(x)\n        y=x\n        x=x+y.view(x.shape)\n        y=x\n        conv2 = self.conv2(conv1)\n        x=x+y.view(x.shape)\n        y=x\n        conv3 = self.conv3(conv2)\n        x=x+y.view(x.shape)\n        y=x\n        conv4 = self.conv4(conv3)\n        x=x+y.view(x.shape)\n        y=x\n        # conv5 = self.conv5(conv4)\n\n        # Side outputs\n        side1 = self.decoder1(conv1)\n        side2 = self.decoder2(conv2)\n        side3 = self.decoder3(conv3)\n        side4 = self.decoder4(conv4)\n        side5=self.linear(side5)\n        side5=side5.view((1,1,32,32))\n        side5=self.decoder5(side5)\n\n        # side5 = self.side5(conv5)\n\n        # print(side1.shape,side2.shape,side3.shape,side4.shape)\n        # Upsampling\n        # side1 = F.interpolate(side1, size=(h, w), mode='bicubic', align_corners=True)\n        # side2 = F.interpolate(side2, size=(h, w), mode='bicubic', align_corners=True)\n        # side3 = F.interpolate(side3, size=(h, w), mode='bicubic', align_corners=True)\n        # side4 = F.interpolate(side4, size=(h, w), mode='bicubic', align_corners=True)\n        # batch_size, channels, height, width = side1.shape\n        # flattened_tensor = side1.view(batch_size, -1)  # Flatten to 1D\n        # fc_layer = nn.Linear(flattened_tensor.shape[1], flattened_tensor.shape[1])  # Input size = Output size\n        # fc_output = fc_layer(flattened_tensor.to(device))\n        # side1 = fc_output.view(batch_size, channels, height, width)\n        # batch_size, channels, height, width = side2.shape\n        # flattened_tensor = side2.view(batch_size, -1)  # Flatten to 1D\n        # fc_layer = nn.Linear(flattened_tensor.shape[1], flattened_tensor.shape[1])  # Input size = Output size\n        # fc_output = fc_layer(flattened_tensor.to(device))\n        # side2 = fc_output.view(batch_size, channels, height, width)\n        # batch_size, channels, height, width = side3.shape\n        # flattened_tensor = side3.view(batch_size, -1)  # Flatten to 1D\n        # fc_layer = nn.Linear(flattened_tensor.shape[1], flattened_tensor.shape[1])  # Input size = Output size\n        # fc_output = fc_layer(flattened_tensor.to(device))\n        # side3 = fc_output.view(batch_size, channels, height, width)\n        # batch_size, channels, height, width = side4.shape\n        # flattened_tensor = side1.view(batch_size, -1)  # Flatten to 1D\n        # fc_layer = nn.Linear(flattened_tensor.shape[1], flattened_tensor.shape[1])  # Input size = Output size\n        # fc_output = fc_layer(flattened_tensor.to(device))\n        # side4 = fc_output.view(batch_size, channels, height, width)\n        # side5 = F.interpolate(side5, size=(h, w), mode='bilinear', align_corners=True)\n\n        # Fusion of side outputs\n        # print(side1.shape,side2.shape,side3.shape,side4.shape,side5.shape)\n        fused = torch.cat((side1, side2, side3, side4,side5), dim=1)\n        fused = self.fusion(fused)\n\n        # Apply sigmoid to all outputs\n        # side1 = torch.sigmoid(side1)\n        # side2 = torch.sigmoid(side2)\n        # side3 = torch.sigmoid(side3)\n        # side4 = torch.sigmoid(side4)\n        # side5 = torch.sigmoid(side5)\n        # fused = torch.sigmoid(fused)\n\n        return side1, side2, side3, side4,side5, fused\n\n\n\n# HED Loss\n# def hed_loss(side_outputs, fused, labels, beta=1.1):\n#     # Compute loss for each side output\n\n#     loss1 = depth_smoothness_loss(side_outputs[0], labels)\n#     loss2 =  depth_smoothness_loss(side_outputs[1], labels)\n#     loss3 = depth_smoothness_loss(side_outputs[2], labels)\n#     loss4 = depth_smoothness_loss(side_outputs[3], labels)\n#     # loss5 =  depth_smoothness_loss(side_outputs[4], labels)\n\n#     # Compute loss for fused output\n#     loss_fused =depth_smoothness_loss(fused, labels)\n\n#     # Total loss\n#     loss_total = loss1 + loss2 + loss3 + loss4 +  loss_fused\n\n#     return loss_total\ndef hed_loss(side_outputs, fused, labels, beta=1.1):\n    # Compute loss for each side output\n    MSELoss=nn.MSELoss()\n    loss1 = 1 - ssim(side_outputs[0], labels, data_range=1, size_average=True)+MSELoss(side_outputs[0], labels)#+ depth_smoothness_loss(side_outputs[0], labels)\n    loss2 =  1 - ssim(side_outputs[1], labels, data_range=1, size_average=True)+MSELoss(side_outputs[1], labels)#+ depth_smoothness_loss(side_outputs[1], labels)\n    loss3 =  1- ssim(side_outputs[2], labels, data_range=1, size_average=True)+MSELoss(side_outputs[2], labels)#+ depth_smoothness_loss(side_outputs[2], labels)\n    loss4 =  1- ssim(side_outputs[3], labels, data_range=1, size_average=True)+MSELoss(side_outputs[3], labels)#+ depth_smoothness_loss(side_outputs[3], labels)\n    loss5 = 1 - ssim(side_outputs[4], labels, data_range=1, size_average=True)+MSELoss(side_outputs[4], labels)#+ depth_smoothness_loss(side_outputs[0], labels)\n\n    # loss5 =  MSELoss(side_outputs[4], labels)\n\n    # Compute loss for fused output\n    loss_fused =1 - ssim(fused, labels, data_range=1, size_average=True)+MSELoss(fused, labels)#+depth_smoothness_loss(fused, labels)\n\n    # Total loss\n    loss_total = loss1 + loss2 + loss3 + loss4 + loss5+ loss_fused\n\n    return loss_total","metadata":{"trusted":true,"id":"coDV3NsDFCTr","execution":{"iopub.status.busy":"2025-04-29T09:03:47.989389Z","iopub.status.idle":"2025-04-29T09:03:47.990307Z","shell.execute_reply.started":"2025-04-29T09:03:47.989553Z","shell.execute_reply":"2025-04-29T09:03:47.989568Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nclass HED(nn.Module):\n    def __init__(self):\n        super(HED, self).__init__()\n\n        # Load pre-trained VGG16 features\n        features = models.vgg16(pretrained=False).features\n\n        # Split VGG16 into blocks (until each pooling layer)\n        self.conv0 = nn.Conv2d(5,3, kernel_size=1)\n        self.conv1 = features[:4]   # until pool1\n        self.conv2 = features[4:9]  # until pool2\n        self.conv3 = features[9:16] # until pool3\n        self.conv4 = features[16:23] # until pool4\n        self.conv5 = features[23:30] # until pool5 (exclude pool5)\n\n        # Side output layers\n        self.side1 = nn.Conv2d(64, 1, kernel_size=1)\n        self.side2 = nn.Conv2d(256, 1, kernel_size=1)\n        self.side3 = nn.Conv2d(256, 1, kernel_size=1)\n        self.side4 = nn.Conv2d(512, 1, kernel_size=1)\n        self.side5 = nn.Conv2d(512, 1, kernel_size=1)\n\n        # Fusion layer\n        self.fusion = nn.Conv2d(5, 1, kernel_size=1)\n\n        # Initialize weights for side outputs and fusion\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        h, w = x.size(2), x.size(3)\n\n        # VGG blocks forward pass\n        x=self.conv0(x)\n        conv1 = self.conv1(x)\n        conv2 = self.conv2(conv1)\n        conv3 = self.conv3(conv2)\n        conv4 = self.conv4(conv3)\n        conv5 = self.conv5(conv4)\n\n        # Side outputs\n        side1 = self.side1(conv1)\n        side2 = self.side2(conv2)\n        side3 = self.side3(conv3)\n        side4 = self.side4(conv4)\n        side5 = self.side5(conv5)\n\n        # Upsampling\n        side1 = F.interpolate(side1, size=(h, w), mode='bilinear', align_corners=True)\n        side2 = F.interpolate(side2, size=(h, w), mode='bilinear', align_corners=True)\n        side3 = F.interpolate(side3, size=(h, w), mode='bilinear', align_corners=True)\n        side4 = F.interpolate(side4, size=(h, w), mode='bilinear', align_corners=True)\n        side5 = F.interpolate(side5, size=(h, w), mode='bilinear', align_corners=True)\n\n        # Fusion of side outputs\n        fused = torch.cat((side1, side2, side3, side4, side5), dim=1)\n        fused = self.fusion(fused)\n\n        # Apply sigmoid to all outputs\n        # side1 = torch.sigmoid(side1)\n        # side2 = torch.sigmoid(side2)\n        # side3 = torch.sigmoid(side3)\n        # side4 = torch.sigmoid(side4)\n        # side5 = torch.sigmoid(side5)\n        # fused = torch.sigmoid(fused)\n\n        return side1, side2, side3, side4, side5, fused\n\n\n\n# HED Loss\ndef hed_loss(side_outputs, fused, labels, beta=1.1):\n    # Compute loss for each side output\n    MSELoss=nn.MSELoss()\n    loss1 = MSELoss(side_outputs[0], labels)\n    loss2 =  MSELoss(side_outputs[1], labels)\n    loss3 = MSELoss(side_outputs[2], labels)\n    loss4 = MSELoss(side_outputs[3], labels)\n    loss5 =  MSELoss(side_outputs[4], labels)\n\n    # Compute loss for fused output\n    loss_fused =MSELoss(fused, labels)\n\n    # Total loss\n    loss_total = loss1 + loss2 + loss3 + loss4 + loss5 + loss_fused\n\n    return loss_total","metadata":{"trusted":true,"id":"uWH3Ni3hFCTs","execution":{"iopub.status.busy":"2025-04-29T09:03:47.991966Z","iopub.status.idle":"2025-04-29T09:03:47.992749Z","shell.execute_reply.started":"2025-04-29T09:03:47.992183Z","shell.execute_reply":"2025-04-29T09:03:47.992207Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"id":"mPwLxLwBFCTs"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare dataset and dataloader\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nlossesh=nn.MSELoss()\n# Training loop\nmodel = HED().to(device)\nprint('j')\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\nprint('j')\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    val_loss=0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        # print('i')\n        # Forward pass\n        side1, side2, side3, side4,side5, fused = model(inputs)\n\n        # Compute loss\n\n        loss = hed_loss([side1, side2, side3, side4,side5], fused, labels)\n        loss2 = lossesh(fused, labels)\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss2.item()\n    # for inputs, labels in train_loader_2:\n    #     inputs, labels = inputs.to(device), labels.to(device)\n    #     # print('i')\n    #     # Forward pass\n    #     side1, side2, side3, side4, fused = model(inputs)\n\n    #     # Compute loss\n\n    #     loss = hed_loss([side1, side2, side3, side4], fused, labels)\n    #     loss2 = lossesh(fused, labels)\n\n    #     # Backward and optimize\n    #     optimizer.zero_grad()\n    #     loss.backward()\n    #     optimizer.step()\n\n    #     running_loss += loss2.item()\n    for inputs, labels in val_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        # Forward pass\n        side1, side2, side3, side4,side5,  fused = model(inputs)\n\n        # Compute loss\n\n        loss = lossesh(fused, labels)\n\n\n\n        val_loss += loss.item()\n    print('val',(val_loss/len(val_loader))**0.5)\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {((running_loss/(len(train_loader)))**0.5):.4f}\")","metadata":{"trusted":true,"id":"NI5NHU_wFCTs","outputId":"4a7d53a5-7adf-443c-91f7-7c647c5041d0","execution":{"iopub.status.busy":"2025-04-29T09:03:48.062664Z","iopub.execute_input":"2025-04-29T09:03:48.063238Z","iopub.status.idle":"2025-04-29T09:03:48.077497Z","shell.execute_reply.started":"2025-04-29T09:03:48.063207Z","shell.execute_reply":"2025-04-29T09:03:48.071010Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# import os\n\n#   # Replace with your folder path\n\n# # Iterate through the folder\n# for image_name in os.listdir('/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/testing-images'):\n#     if image_name.endswith('.jpg') or image_name.endswith('.png'):  # Check for image files\n#         image_path = os.path.join('/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/testing-images', image_name)\n\n#         # Load and preprocess the imag`e\n#         image = Image.open(image_path)\n#         input_tensor = transform(image).unsqueeze(0)\n\n#         # Perform inference\n#         with torch.no_grad():\n#             output = model(input_tensor.to(device))\n#             output_image=(output[5][:][:][0][0]*255).cpu().numpy()\n#             output_pil = Image.fromarray(output_image).convert('L')\n#             output_pil.save('test_fold/'+image_name.split('.')[0]+'.jpg')\n#              # Get the predicted class\n\n#         # print(f\"Image: {image_name}\")\n","metadata":{"trusted":true,"id":"7IeG9T9AFCTt","execution":{"iopub.status.busy":"2025-04-29T09:03:48.079272Z","iopub.execute_input":"2025-04-29T09:03:48.079501Z","iopub.status.idle":"2025-04-29T09:03:48.097867Z","shell.execute_reply.started":"2025-04-29T09:03:48.079478Z","shell.execute_reply":"2025-04-29T09:03:48.093647Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true,"id":"hJOGSXA4FCTt"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"id":"VIA9xVTIFCTt"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ssim_module = SSIM(data_range=1, size_average=True, channel=3)\n","metadata":{"trusted":true,"id":"v6g8oLmeFCTt","execution":{"iopub.status.busy":"2025-04-29T09:03:48.099983Z","iopub.execute_input":"2025-04-29T09:03:48.100219Z","iopub.status.idle":"2025-04-29T09:03:48.125344Z","shell.execute_reply.started":"2025-04-29T09:03:48.100190Z","shell.execute_reply":"2025-04-29T09:03:48.121159Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true,"id":"J2eXJTA6FCTt"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip install SSIM","metadata":{"trusted":true,"id":"fCnmEyJOFCTt","execution":{"iopub.status.busy":"2025-04-29T09:03:48.127211Z","iopub.execute_input":"2025-04-29T09:03:48.127569Z","iopub.status.idle":"2025-04-29T09:03:48.147227Z","shell.execute_reply.started":"2025-04-29T09:03:48.127547Z","shell.execute_reply":"2025-04-29T09:03:48.142959Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# from pytorch_msssim import ssim, ms_ssim","metadata":{"trusted":true,"id":"jQTcePTdFCTt","execution":{"iopub.status.busy":"2025-04-29T09:03:48.149002Z","iopub.execute_input":"2025-04-29T09:03:48.149405Z","iopub.status.idle":"2025-04-29T09:03:48.165880Z","shell.execute_reply.started":"2025-04-29T09:03:48.149381Z","shell.execute_reply":"2025-04-29T09:03:48.160627Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"pip install cv2","metadata":{"trusted":true,"id":"5-6mFsAYFCTt","outputId":"391b49fd-3a86-4e44-b4d2-e4565b2b4091","execution":{"iopub.status.busy":"2025-04-29T09:03:48.168155Z","iopub.execute_input":"2025-04-29T09:03:48.168379Z","iopub.status.idle":"2025-04-29T09:03:50.168355Z","shell.execute_reply.started":"2025-04-29T09:03:48.168359Z","shell.execute_reply":"2025-04-29T09:03:50.163474Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement cv2 (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for cv2\u001b[0m\u001b[31m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# import torch\n# import torch.nn.functional as F\n\n# def depth_smoothness_loss(depth, image):\n#     \"\"\"\n#     Computes the depth smoothness loss.\n\n#     Args:\n#         depth (torch.Tensor): Predicted depth map of shape (B, 1, H, W).\n#         image (torch.Tensor): Input image of shape (B, 3, H, W).\n\n#     Returns:\n#         torch.Tensor: Smoothness loss.\n#     \"\"\"\n#     # Compute gradients of the depth map\n#     depth_dx = torch.abs(depth[:, :, :, :-1] - depth[:, :, :, 1:])\n#     depth_dy = torch.abs(depth[:, :, :-1, :] - depth[:, :, 1:, :])\n\n#     # Compute gradients of the image\n#     image_dx = torch.mean(torch.abs(image[:, :, :, :-1] - image[:, :, :, 1:]), dim=1, keepdim=True)\n#     image_dy = torch.mean(torch.abs(image[:, :, :-1, :] - image[:, :, 1:, :]), dim=1, keepdim=True)\n\n#     # Weight depth gradients by image gradients\n#     weights_x = torch.exp(-image_dx)\n#     weights_y = torch.exp(-image_dy)\n\n#     smoothness_x = depth_dx * weights_x\n#     smoothness_y = depth_dy * weights_y\n\n#     return torch.mean(smoothness_x) + torch.mean(smoothness_y)\n","metadata":{"trusted":true,"id":"3HP5mK3NFCTu","execution":{"iopub.status.busy":"2025-04-29T09:03:50.171090Z","iopub.execute_input":"2025-04-29T09:03:50.171394Z","iopub.status.idle":"2025-04-29T09:03:50.180351Z","shell.execute_reply.started":"2025-04-29T09:03:50.171365Z","shell.execute_reply":"2025-04-29T09:03:50.176856Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"train_dataset[0][0].shape","metadata":{"trusted":true,"id":"sXiQNUVDFCTu","execution":{"iopub.status.busy":"2025-04-29T09:03:50.183438Z","iopub.execute_input":"2025-04-29T09:03:50.183732Z","iopub.status.idle":"2025-04-29T09:03:50.226729Z","shell.execute_reply.started":"2025-04-29T09:03:50.183708Z","shell.execute_reply":"2025-04-29T09:03:50.221930Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_dataset\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n","\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"],"ename":"NameError","evalue":"name 'train_dataset' is not defined","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom torch.nn import Conv2d, BatchNorm2d,ReLU,MaxPool2d\n\nclass HED(nn.Module):\n    def __init__(self):\n        super(HED, self).__init__()\n        self.conv0=Conv2d(1, 64, kernel_size=(7, 7), stride=(1, 1), padding='same', bias=False)\n        self.bn0=BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        self.relu0=ReLU(inplace=True)\n        self.layer20=nn.Sequential(\n\n\n       Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False,padding='same'),\n        BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n      )\n        self.layer30=nn.Sequential(\n    Conv2d(256, 1024, kernel_size=(1, 1),stride=(1, 1), bias=False,padding='same'),\n        BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n      )\n        self.layer40=nn.Sequential(\n    Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False,padding='same'),\n        BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n      )\n        self.layer11=nn.Sequential(\n\n             Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n               BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n              ReLU(inplace=True),\n              Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n             BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n        )\n        self.layer12=nn.Sequential(\n\n             Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n               BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n              ReLU(inplace=True),\n              Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n             BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n        )\n        self.layer13=nn.Sequential(\n\n             Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n               BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n              ReLU(inplace=True),\n              Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n             BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n        )\n\n        self.layer21=nn.Sequential(\n\n             Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n               BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n              ReLU(inplace=True),\n              Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n             BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n        )\n        self.layer22=nn.Sequential(\n\n             Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n               BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n              ReLU(inplace=True),\n              Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n             BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n        )\n        self.layer23=nn.Sequential(\n\n             Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n               BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n              ReLU(inplace=True),\n              Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n             BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n        )\n        self.layer24=nn.Sequential(\n\n             Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n               BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n              ReLU(inplace=True),\n              Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n             BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n        )\n        self.layer31=nn.Sequential(\n\n             Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n               BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n              ReLU(inplace=True),\n              Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n             BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n        )\n        self.layer32=nn.Sequential(\n\n             Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n               BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n              ReLU(inplace=True),\n              Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n             BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n        )\n\n        self.layer33=nn.Sequential(\n\n             Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n               BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n              ReLU(inplace=True),\n              Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n             BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n        )\n        self.layer34=nn.Sequential(\n\n             Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n               BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n              ReLU(inplace=True),\n              Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n             BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n        )\n        self.layer35=nn.Sequential(\n\n             Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n               BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n              ReLU(inplace=True),\n              Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n             BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n        )\n        self.layer36=nn.Sequential(\n\n             Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n               BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n              ReLU(inplace=True),\n              Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n             BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n        )\n        self.layer41=nn.Sequential(\n\n             Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n               BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n              ReLU(inplace=True),\n              Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n             BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n        )\n        self.layer42=nn.Sequential(\n\n             Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n               BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n              ReLU(inplace=True),\n              Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n             BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n        )\n        self.layer43=nn.Sequential(\n\n             Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n               BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n              ReLU(inplace=True),\n              Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n             BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n        )\n\n\n        # self.conv5 = features[23:30] # until pool5 (exclude pool5)\n        # self.linear=nn.Linear(1000,1024)\n        # # Side output layers\n        # self.side1 = nn.Conv2d(64, 1, kernel_size=1)\n        # self.side2 = nn.Conv2d(128, 1, kernel_size=1)\n        # self.side3 = nn.Conv2d(256, 1, kernel_size=1)\n        # self.side4 = nn.Conv2d(512, 1, kernel_size=1)\n\n        # self.side5 = nn.Conv2d(512, 1, kernel_size=1)\n\n        # Fusion layer\n         # Upscale x2\n\n        self.decoder1= nn.Sequential(\n            nn.ConvTranspose2d(in_channels= 1024,out_channels=1024, kernel_size=3, stride=1, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(in_channels= 1024,out_channels=1024, kernel_size=3, stride=1, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True))\n        self.decoder2= nn.Sequential(\n            nn.ConvTranspose2d(in_channels= 1024,out_channels=512, kernel_size=3, stride=1, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(in_channels= 512,out_channels=256, kernel_size=3, stride=1, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True))\n        self.decoder3= nn.Sequential(\n            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True))\n        self.decoder4= nn.Sequential(\n            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True),\n        )\n        self.decoder5= nn.Sequential(\n            nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True))\n        self.decoder6=nn.Sequential(\n            nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=3, stride=1, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(in_channels=16, out_channels=8, kernel_size=3, stride=1, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True))\n        self.decoder7=nn.Sequential(\n            nn.ConvTranspose2d(in_channels=8, out_channels=8, kernel_size=3, stride=1, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(in_channels=8, out_channels=8, kernel_size=3, stride=1, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True)\n        )\n        self.decoder8=nn.Sequential(\n            nn.ConvTranspose2d(in_channels=8, out_channels=4, kernel_size=3, stride=1, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(in_channels=4, out_channels=2, kernel_size=3, stride=1, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True)\n        )\n        self.decoder9=nn.Sequential(\n            nn.ConvTranspose2d(in_channels=2, out_channels=2, kernel_size=3, stride=1, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(in_channels=2, out_channels=1, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1),# Upscale x2\n            nn.ReLU(inplace=True))\n\n        self.decoder10= nn.Sequential(\n            nn.ConvTranspose2d(in_channels= 1024,out_channels=32, kernel_size=3, stride=1, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(in_channels= 32,out_channels=1, kernel_size=3, stride=1, padding=1),  # Upscale x2\n            nn.ReLU(inplace=True))\n\n        # Initialize weights for side outputs and fusion\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        h, w = x.size(2), x.size(3)\n\n        # VGG blocks forward pass\n\n\n\n        x=self.conv0(x)\n        x=self.bn0(x)\n        x=self.relu0(x)\n        conv1=self.layer11(x)\n        x=x+conv1\n        conv1=self.layer12(x)\n        x=x+conv1\n        conv1=self.layer13(x)\n        x=x+conv1\n        conv1=x\n        conv2=self.layer20(x)\n        x=conv2\n        conv2=self.layer21(conv2)\n        x=x.view(conv2.shape)+conv2\n        conv2=self.layer22(x)\n        x=x+conv2\n        conv2=self.layer23(x)\n        x=x+conv2\n        conv2=self.layer24(x)\n        x=x+conv2\n        conv2=x\n        conv3=self.layer30(x)\n        x=conv3\n        conv3=self.layer31(conv3)\n        x=x.view(conv3.shape)+conv3\n        conv3=self.layer32(x)\n        x=x+conv3\n        conv3=self.layer33(x)\n        x=x+conv3\n        conv3=self.layer34(x)\n        x=x+conv3\n        conv3=self.layer35(x)\n        x=x+conv3\n        conv3=self.layer36(x)\n        x=x+conv3\n        conv3=x\n        conv4=self.layer40(x)\n        x=conv4\n        conv4=self.layer41(conv4)\n        x=x.view(conv4.shape)+conv4\n        conv4=self.layer42(x)\n        x=x+conv4\n        conv4=self.layer43(x)\n        x=x+conv4\n        # conv4=self.decoder1(conv4)\n       \n        # conv4=self.decoder2(conv4)\n      \n        # conv4=self.decoder3(conv4)\n   \n        # conv4=self.decoder4(conv4)\n     \n        # conv4=self.decoder5(conv4)\n     \n        # conv4=self.decoder6(conv4)\n\n        # conv4=self.decoder7(conv4)\n     \n        # conv4=self.decoder8(conv4)\n        # conv4=self.decoder9(conv4)\n        conv4=decoder10(conv4)\n        # conv5 = self.conv5(conv4)\n        # for layer in self.decoder3:\n        #  conv3 = layer(conv3)\n        #  print(conv3.shape)\n\n        # side5 = self.side5(conv5)\n\n\n\n\n        # Apply sigmoid to all outputs\n        # side1 = torch.sigmoid(side1)\n        # side2 = torch.sigmoid(side2)\n        # side3 = torch.sigmoid(side3)\n        # side4 = torch.sigmoid(side4)\n        # side5 = torch.sigmoid(side5)\n        # fused = torch.sigmoid(fused)\n\n        return conv4\n\n\n\n# HED Loss\n# def hed_loss(side_outputs, fused, labels, beta=1.1):\n#     # Compute loss for each side output\n\n#     loss1 = depth_smoothness_loss(side_outputs[0], labels)\n#     loss2 =  depth_smoothness_loss(side_outputs[1], labels)\n#     loss3 = depth_smoothness_loss(side_outputs[2], labels)\n#     loss4 = depth_smoothness_loss(side_outputs[3], labels)\n#     # loss5 =  depth_smoothness_loss(side_outputs[4], labels)\n\n#     # Compute loss for fused output\n#     loss_fused =depth_smoothness_loss(fused, labels)\n\n#     # Total loss\n#     loss_total = loss1 + loss2 + loss3 + loss4 +  loss_fused\n\n#     return loss_total\n","metadata":{"trusted":true,"id":"JPFoVLIBFCTu","execution":{"iopub.status.busy":"2025-04-29T09:03:50.229352Z","iopub.status.idle":"2025-04-29T09:03:50.231713Z","shell.execute_reply.started":"2025-04-29T09:03:50.229529Z","shell.execute_reply":"2025-04-29T09:03:50.229545Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def lossesh(ytrue,ypred):\n    mse=nn.MSELoss()\n    mae=nn.L1Loss()\n    l1var=torch.mean(torch.abs(ytrue-torch.mean((ytrue))))\n    l2var=l2var = torch.std(ytrue, unbiased=False)\n    return torch.nan_to_num((2*(mse(ytrue,ypred))**0.5+4*mae(ytrue,ypred)-l2var),nan=0)","metadata":{"id":"JLzDc2uDjaR4","trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:03:50.233363Z","iopub.status.idle":"2025-04-29T09:03:50.235388Z","shell.execute_reply.started":"2025-04-29T09:03:50.233547Z","shell.execute_reply":"2025-04-29T09:03:50.233562Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training loop\nmodel = HED().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:03:50.237381Z","iopub.status.idle":"2025-04-29T09:03:50.238420Z","shell.execute_reply.started":"2025-04-29T09:03:50.237555Z","shell.execute_reply":"2025-04-29T09:03:50.237569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Trainable Parameters: {trainable_params}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:03:50.240965Z","iopub.status.idle":"2025-04-29T09:03:50.242506Z","shell.execute_reply.started":"2025-04-29T09:03:50.241158Z","shell.execute_reply":"2025-04-29T09:03:50.241172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nig=nn.MSELoss()","metadata":{"id":"0cRLRFO6jiOG","trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:03:50.244057Z","iopub.status.idle":"2025-04-29T09:03:50.244568Z","shell.execute_reply.started":"2025-04-29T09:03:50.244269Z","shell.execute_reply":"2025-04-29T09:03:50.244284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare dataset and dataloader\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nimport torch_xla.distributed.parallel_loader as pl\n\nlossesh1=nn.MSELoss()\n\nprint('j')\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\nprint('j')\nnum_epochs = 50\nfor epoch in range(5):\n    model.train()\n    running_loss = 0.0\n    val_loss=0\n    model.train()\n    train_sampler.set_epoch(epoch)  # Ensure proper data distribution across TPU cores\n    para_loader = pl.ParallelLoader(train_loader, [device])\n    i=0\n    for batch in para_loader.per_device_loader(device):\n        inputs, labels =batch\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        # print(i)\n        i+=1\n        # Forward pass\n        fused = model(inputs)\n\n        # Compute loss\n\n\n        loss = lossesh(fused, labels)\n        loss2=lossesh1(fused, labels)\n        # print(loss2**0.5)\n        # # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        xm.optimizer_step(optimizer)\n\n        running_loss += loss2.item()\n    # for inputs, labels in train_loader_2:\n    #     inputs, labels = inputs.to(device), labels.to(device)\n    #     # print('i')\n    #     # Forward pass\n    #     side1, side2, side3, side4, fused = model(inputs)\n\n    #     # Compute loss\n\n    #     loss = hed_loss([side1, side2, side3, side4], fused, labels)\n    #     loss2 = lossesh(fused, labels)\n\n    #     # Backward and optimize\n    #     optimizer.zero_grad()\n    #     loss.backward()\n    #     optimizer.step()\n\n    #     running_loss += loss2.item()\n    para_loader = pl.ParallelLoader(val_loader, [device])\n    for batch in para_loader.per_device_loader(device):\n        inputs, labels = batch\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        # Forward pass\n        fused = model(inputs)\n\n        # Compute loss\n\n        loss =nig(fused, labels)\n\n\n\n        val_loss += loss.item()\n    print('val',(val_loss/len(val_loader))**0.5)\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {((running_loss/(len(train_loader)))**0.5):.4f}\")","metadata":{"id":"MSRvZ1cMjlgF","outputId":"bcbf161e-81a7-4c1e-d5bd-2fbe87bbe6ce","trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:03:50.246459Z","iopub.status.idle":"2025-04-29T09:03:50.247106Z","shell.execute_reply.started":"2025-04-29T09:03:50.246766Z","shell.execute_reply":"2025-04-29T09:03:50.246781Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Create a folder named 'my_folder' in the working directory\nos.makedirs(\"/kaggle/working/test_fold\", exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:03:50.247787Z","iopub.status.idle":"2025-04-29T09:03:50.248694Z","shell.execute_reply.started":"2025-04-29T09:03:50.247923Z","shell.execute_reply":"2025-04-29T09:03:50.247936Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n  # Replace with your folder path\n\n# Iterate through the folder\nfor image_name in os.listdir('/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/testing-images'):\n    if image_name.endswith('.jpg') or image_name.endswith('.png'):  # Check for image files\n        image_path = os.path.join('/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/testing-images', image_name)\n\n        # Load and preprocess the imag`e\n        image = Image.open(image_path)\n        input_tensor = transform(image).unsqueeze(0)\n\n        # Perform inference\n        with torch.no_grad():\n            output = model(input_tensor.to(device))\n            output_image=(output[:][:][0][0]*255).cpu().numpy()\n            output_pil = Image.fromarray(output_image).convert('L')\n            output_pil.save('test_fold/'+image_name.split('.')[0]+'.jpg')\n             # Get the predicted class\n\n        # print(f\"Image: {image_name}\")\n","metadata":{"id":"u1xf1yqdlHQP","outputId":"ddb98522-5fdd-4a06-fed4-3f8bc8cb5358","trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:03:50.249524Z","iopub.status.idle":"2025-04-29T09:03:50.250235Z","shell.execute_reply.started":"2025-04-29T09:03:50.249689Z","shell.execute_reply":"2025-04-29T09:03:50.249705Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport pandas as pd\nimport numpy as np\n\ndef images_to_csv_with_metadata(image_folder, output_csv):\n    # Initialize an empty list to store image data and metadata\n    data = []\n\n    # Loop through all images in the folder\n    for idx, filename in enumerate(sorted(os.listdir(image_folder))):\n        # print(filename.endswith(\".png\"))\n        # if filename.endswith(\".png\"):\n\n            # print(filename)\n            filepath = os.path.join(image_folder, filename)\n            # Read the image\n            image = cv2.imread(filepath, cv2.IMREAD_UNCHANGED)\n            image = cv2.resize(image, (256, 256))\n            image = image / 255.\n            image = (image - np.min(image)) / (np.max(image) - np.min(image) + 1e-6)\n            image = np.uint8(image * 255.)\n            # Flatten the image into a 1D array\n            image_flat = image.flatten()\n            # Add ID, ImageID (filename), and pixel values\n            row = [idx, filename] + image_flat.tolist()\n            data.append(row)\n\n    # Create a DataFrame\n    num_columns = len(data[0]) - 2 if data else 0\n    column_names = [\"id\", \"ImageID\"] + [indx for indx in range(num_columns)]\n    df = pd.DataFrame(data, columns=column_names)\n\n    # Save to CSV\n    df.to_csv(output_csv, index=False)\n\n# Paths for prediction and ground truth images\npredictions_folder = \"test_fold\"\n\n# Output CSV paths\npredictions_csv = \"predictions_hed_resss_cub7.csv\"\n\n# Convert prediction images to CSV\nimages_to_csv_with_metadata(predictions_folder, predictions_csv)","metadata":{"id":"2B36iNoAsOQP","outputId":"183ad51d-fa47-46fc-d2da-7c02fe507f11","trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:03:50.251153Z","iopub.status.idle":"2025-04-29T09:03:50.251636Z","shell.execute_reply.started":"2025-04-29T09:03:50.251286Z","shell.execute_reply":"2025-04-29T09:03:50.251299Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('jig')","metadata":{"id":"7iLEbNa_sURA","trusted":true,"execution":{"iopub.status.busy":"2025-04-29T09:03:50.252333Z","iopub.status.idle":"2025-04-29T09:03:50.252999Z","shell.execute_reply.started":"2025-04-29T09:03:50.252695Z","shell.execute_reply":"2025-04-29T09:03:50.252711Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}